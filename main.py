import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.dummy import DummyRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# =========================
# 1. LOAD DATA
# =========================

# Ch·ªânh l·∫°i path n·∫øu c·∫ßn
X_train = pd.read_csv("archive/train/x_train.csv")
X_train_BIC = pd.read_csv("archive/train/x_train_BIC.csv")
y_train = pd.read_csv("archive/train/y_train.csv").squeeze()


X_val = pd.read_csv("archive/val/x_val.csv")
X_val_BIC = pd.read_csv("archive/val/x_val_BIC.csv")
y_val = pd.read_csv("archive/val/y_val.csv").squeeze()


print("Train shape:", X_train.shape, "Val shape:", X_val.shape)

# =========================
# 2. KHAI B√ÅO 4 MODEL
# =========================
baseline = Pipeline(
    steps=[
        ("scaler", StandardScaler(with_mean=False)),
        ("model", LinearRegression())
    ]
)

ridge = Pipeline(
    steps=[
        ("scaler", StandardScaler(with_mean=False)),
        ("model", Ridge(alpha=33.6, random_state=42))
    ]
)

lasso = Pipeline(
    steps=[
        ("scaler", StandardScaler(with_mean=False)),
        ("model", Lasso(alpha=78.48, random_state=42, max_iter=10000))
    ]
)

elastic_net = Pipeline(
    steps=[
        ("scaler", StandardScaler(with_mean=False)),
        ("model", ElasticNet(alpha=1.129, l1_ratio=0.7, random_state=42, max_iter=10000))
    ]
)

models = {
    "baseline_linear": baseline,
    "ridge": ridge,
    "lasso": lasso,
    "elasticnet": elastic_net,
}

# =========================
# 3. TRAIN + EVALUATE
# =========================

results = []          # l∆∞u l·∫°i b·∫£ng metrics
predictions = {}      # l∆∞u y_pred ƒë·ªÉ v·∫Ω h√¨nh

for name, model in models.items():
    if (name == "baseline_linear"):
        X_train_used = X_train_BIC
        X_val_used = X_val_BIC
    else:
        X_train_used = X_train
        X_val_used = X_val
    print(f"\n=== Training model: {name} ===")
    model.fit(X_train_used, y_train)
    
    y_val_pred = model.predict(X_val_used)
    predictions[name] = y_val_pred
    
    mae = mean_absolute_error(y_val, y_val_pred)
    rmse = mean_squared_error(y_val, y_val_pred) ** 0.5
    r2 = r2_score(y_val, y_val_pred)
    
    results.append({
        "model": name,
        "MAE": mae,
        "RMSE": rmse,
        "R2": r2,
    })
    
    print(f"Validation MAE:  {mae:,.4f}")
    print(f"Validation RMSE: {rmse:,.4f}")
    print(f"Validation R¬≤:   {r2:,.4f}")

# ƒê∆∞a v·ªÅ DataFrame cho d·ªÖ xem
results_df = pd.DataFrame(results).set_index("model")
print("\n=== Summary metrics on validation set ===")
print(results_df)

# L∆∞u ra CSV n·∫øu mu·ªën ch√®n v√†o report
results_df.to_csv("model_metrics_val.csv")

# =========================
# 4. V·∫º H√åNH SO S√ÅNH
# =========================

# T·∫°o th∆∞ m·ª•c l∆∞u h√¨nh (n·∫øu mu·ªën)
import os
os.makedirs("figures", exist_ok=True)

# 4.1: Predicted vs Actual cho t·ª´ng model
for name, y_pred in predictions.items():
    plt.figure(figsize=(6, 6))
    plt.scatter(y_val, y_pred, alpha=0.4)
    # ƒë∆∞·ªùng y = x (d·ª± ƒëo√°n ho√†n h·∫£o)
    min_val = min(y_val.min(), y_pred.min())
    max_val = max(y_val.max(), y_pred.max())
    plt.plot([min_val, max_val], [min_val, max_val], linestyle="--")
    plt.xlabel("Actual y (validation)")
    plt.ylabel("Predicted y")
    plt.title(f"Predicted vs Actual - {name}")
    plt.tight_layout()
    plt.savefig(f"figures/{name}_pred_vs_actual.png", dpi=200)
    # plt.show()  # b·∫≠t d√≤ng n√†y n·∫øu mu·ªën xem tr·ª±c ti·∫øp

# 4.2: Residual plot (residual vs predicted) cho t·ª´ng model
for name, y_pred in predictions.items():
    residuals = y_val - y_pred
    plt.figure(figsize=(6, 4))
    plt.scatter(y_pred, residuals, alpha=0.4)
    plt.axhline(0, linestyle="--")
    plt.xlabel("Predicted y")
    plt.ylabel("Residual (y_true - y_pred)")
    plt.title(f"Residuals vs Predicted - {name}")
    plt.tight_layout()
    plt.savefig(f"figures/{name}_residuals.png", dpi=200)
    # plt.show()

# 4.3: Histogram residuals cho t·ª´ng model
for name, y_pred in predictions.items():
    residuals = y_val - y_pred
    plt.figure(figsize=(6, 4))
    plt.hist(residuals, bins=30)
    plt.xlabel("Residual")
    plt.ylabel("Frequency")
    plt.title(f"Residual distribution - {name}")
    plt.tight_layout()
    plt.savefig(f"figures/{name}_residual_hist.png", dpi=200)
    # plt.show()

# 4.4: Bar chart so s√°nh MAE, RMSE, R2 gi·ªØa c√°c model
models_order = results_df.index.tolist()

plt.figure(figsize=(6, 4))
plt.bar(models_order, results_df["MAE"])
plt.ylabel("MAE") #(lower is better)
plt.title("Model comparison - MAE")
plt.tight_layout()
plt.ylim(2200, 2600)  # ch·ªânh y-axis cho d·ªÖ nh√¨n

plt.savefig("figures/compare_MAE.png", dpi=200)

plt.figure(figsize=(6, 4))
plt.bar(models_order, results_df["RMSE"])
plt.ylabel("RMSE") #(lower is better)
plt.title("Model comparison - RMSE")
plt.tight_layout()
plt.ylim(3400, 4200)  # ch·ªânh y-axis cho d·ªÖ nh√¨n

plt.savefig("figures/compare_RMSE.png", dpi=200)

plt.figure(figsize=(6, 4))
plt.bar(models_order, results_df["R2"])
plt.ylabel("R¬≤") # (higher is better)
plt.title("Model comparison - R¬≤")
plt.tight_layout()
plt.ylim(0.7, 0.9)  # ch·ªânh y-axis cho d·ªÖ nh√¨n

plt.savefig("figures/compare_R2.png", dpi=200)

print("\nAll figures saved to 'figures/' folder and metrics saved to 'model_metrics_val.csv'.")



import os
os.makedirs("models", exist_ok=True)
# ========================================
# 8Ô∏è‚É£ (Tu·ª≥ ch·ªçn) L∆∞u m√¥ h√¨nh t·ªët nh·∫•t
# ========================================
import joblib

joblib.dump(ridge, "./models/ridge_best_model.pkl")
joblib.dump(lasso, "./models/lasso_best_model.pkl")
joblib.dump(elastic_net, "./models/elasticnet_best_model.pkl")
joblib.dump(baseline, "./models/linear_best_model.pkl")

print("\nüíæ ƒê√£ l∆∞u m√¥ h√¨nh t·ªët nh·∫•t c·ªßa t·ª´ng lo·∫°i v√†o file .pkl")