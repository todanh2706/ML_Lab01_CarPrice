\pagebreak
\section{Lý thuyết về các mô hình và lựa chọn}
\subsection{Mô hình Linear Regression}
Linear regression là một mô hình học có giám sát với khả năng tìm mối liên hệ tuyến tính giữa input và output.\\
\textbf{Vấn đề:} Model này nhận vào một vector $x \in \mathbb{R}^{D+1}$ gồm các đặt trưng của một mẫu và trả về giá trị dự đoán $y \in \mathbb{R}$. Ví dụ: Nhận vào các giá trị thuộc tính của một ngôi nhà như Diện tích sàn ($m^2$), số lượng phòng ngủ, khoảng cách tới trung tâm thành phố (m), tuổi nhà (năm) và trả về dự đoán giá của căn nhà đó.\\
\textbf{Đánh giá hiệu năng: }Mô hình Linear Regression thường được đánh giá bằng Mean Square Error (MSE) trên tập train:\\
\[
MSE_{\text{train}} = \frac{1}{N} \| \hat{\mathbf{y}} - \mathbf{y} \|^2 = \frac{1}{N} \sum_{n=1}^{N} (\hat{y}_n - y_n)^2
\]
với $\hat{\mathbf{y}}$ là tập giá trị dự đoán, $\mathbf{y}$ là tập giá trị thực tế của tập train.\\
\textbf{Huấn luyện mô hình}: Các phương pháp huấn luyện mô hình Linear Regression được dựa trên cách tính toán vector trọng số $\omega$ để giảm MSE xuống thấp nhất. Một cách làm phổ biến là tính toán đạo hàm của hàm MSE theo $\omega$ và cho nó bằng 0 để tìm cực tiểu.
\[
\nabla_{\mathbf{w}} (MSE_{\text{train}}) = \nabla_{\mathbf{w}} (\mathbf{w}^T \mathbf{X}^T \mathbf{X} \mathbf{w} - \mathbf{w}^T \mathbf{X}^T \mathbf{y} - \mathbf{y}^T \mathbf{X} \mathbf{w} + \mathbf{y}^T \mathbf{y}) = 2\mathbf{X}^T \mathbf{X} \mathbf{w} - 2\mathbf{X}^T \mathbf{y}
\]

\begin{align*}
\nabla_{\mathbf{w}} (MSE_{\text{train}}) &= 0 \\
\mathbf{X}^T \mathbf{X} \mathbf{w} - \mathbf{X}^T \mathbf{y} &= 0 \\
\mathbf{X}^T \mathbf{X} \mathbf{w} &= \mathbf{X}^T \mathbf{y} \\
\mathbf{w} &= (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
\end{align*}
\subsection{Mô hình Ridge}
\subsubsection{Cơ sở lý thuyết}
Hồi quy Ridge (\textbf{\textit{Ridge Regression}}) là một kỹ thuật hồi quy tuyến tính đuọc điều chỉnh (\textit{regularized}) nhằm giải quyết hiện tượng quá khớp (\textit{overfitting}) bằng cách dùng các kỹ thuật Chính quy hoá (\textit{Regularization}). Phương pháp này thêm một số hạng (thường là \textit{penalty}) vào hàm mất mát (\textit{loss function}) của mô hình. Thành phần phạt này dùng để đánh giá và kiểm soát độ phức tạp của mô hình\\
Cụ thể, mô hình Ridge sử dụng kỹ thuật \textit{L2 Regularization}, thay vì chỉ tối thiểu hoá tổng bình phương sai số (\textit{Mean Square Error - MSE}) như mô hình hồi quy tuyến tính cơ bản thì Ridge sẽ tối thiểu hoá một hàm mục tiêu mới:
\begin{center}
    $\displaystyle J(w)=\frac{1}{2}\|\mathbf{y}-\mathbf{X}\mathbf{w}\|_2^2+\lambda.\|\mathbf{w}\|_2^2$ \cite{Vu_2017}
\end{center}
Trong đó, $\mathbf{w}$ là vector trọng số của mô hình và $\lambda$ là siêu tham số (\textit{hyperparameter}) điều chỉnh độ mạnh của thành phần phạt $\|\mathbf{w}\|_2^2$.\\
\cite{Ridge} Việc thêm thành phần phạt sẽ ép các trọng số của mô hình phải giữ ở mức nhỏ, co về gần 0. Điều này làm giảm sự phụ thuộc của mô hình vào bất kỳ một biến đầu vào cụ thể nào, khiến mô hình đơn giản hơn, ít nhạy cảm với nhiễu dữ liệu và tăng khả năng tổng quát hoá. Vì thế, bài toán tối ưu hàm mất mát của hồi quy Ridge thực chất là tối ưu song song hai thành phần bao gồm tổng bình phương sai số và thành phần phạt hay thành phần điều chuẩn (\textit{regularization term}).
\begin{itemize}
    \item Trường hợp $\lambda=0$, thành phần điều chuẩn bị tiêu giảm và chúng ta quay về hồi quy tuyến tính.
    \item Trường hợp $\lambda \approx 0$, thành phần điều chuẩn trở nên ít quan trọng, mức độ kiểm soát quá khớp trở nên kém.
    \item Trường hợp $\lambda$ lớn, mức độ kiểm soát lên độ lớn của các hệ số ước lượng tăng lên qua đó giảm bớt quá khớp.
\end{itemize}
Khi $\lambda$ tăng dần, hồi quy Ridge có xu hướng thu hẹp hệ số ước lượng $\mathbf{w}$ từ mô hình.
\subsubsection{Triển khai thực nghiệm}
Mô hình Ridge được triển khai và cấu hình như sau:
\begin{itemize}
    \item Pipeline: Mô hình được gói trong một \texttt{sklearn.pipeline.Pipeline} để chuẩn hoá quy trình xử lý.
    \item Chuẩn hoá (Scaling): Bước đầu tiên trong pipeline là chuẩn hoá dữ liệu, rất quan trọng đối với các mô hình được chính quy hoá như Ridge, vì thành phần phạt nhạy cảm với sự chênh lệch về thang đo (\textit{scale}) của các biến đầu vào.
    \item Cấu hình: Mô hình Ridge được khởi tạo với siêu tham số \texttt{alpha=33.6} và \texttt{random\_state=42} (được dùng để đảm bảo kết quả có thể được tái lập).
    \item Dữ liệu: Không giống như mô hình \texttt{baseline\_linear} sử dụng bộ dữ liệu được chọn từ phương pháp chọn lọc thuộc tính ở phần trước, mô hình Ridge được huấn luyện trên bộ dữ liệu đầy đủ (đã được lưu lại vào \texttt{X\_train} và \texttt{y\_train}).
\end{itemize}
Siêu tham số \texttt{alpha} lúc này sẽ được lựa chọn thông qua các bước sau:
\begin{itemize}
    \item Định nghĩa không gian tìm kiếm: Một tập hợp các giá trị \texttt{alpha} tiềm năng được định nghĩa bằng \texttt{numpy.logspace(-4, 3, 20)}, tức là một mảng chứa 20 giá trị thực phân bố theo thang logarit từ $10^{-4}$ đến $10^3$. Việc sử dụng thang logarit cho phép khám phá hiệu qua  các giá trị ở nhiều bậc độ lớn khác nhau.
    \item Đóng gói Pipeline: Dùng chính mô hình Ridge để chuẩn hoá dữ liệu trước khi huấn luyện mô hình, mục tiêu là tìm ra và đánh giá $R^2$.
    \item Kiểm định chéo (\textit{Cross-Validation}): \texttt{GridSearchCV} được cấu hình để sử dụng phương pháp kiểm định chéo. Toàn bộ tập dữ liệu huấn luyện sẽ được chia thành 5 phần (\textit{folds}). Quá trình tìm kiếm sẽ lặp lại 5 lần, mỗi lần sẽ có một phàn được giữ lại làm tập validation tạm thời và 4 phần còn lại sẽ dùng để huấn luyện.
    \item Tiêu chí đánh giá: Chỉ số được sử dụng để đánh giá là $R^2$
    \item Lựa chọn tối ưu: quy trình \texttt{GridSearchCV} tự động huấn luyện và đánh giá mô hình Ridge với từng giá trị \texttt{alpha} trong không gian tìm kiếm. Giá trị nào mang $R^2$ trung bình cao nhất (qua 5 lượt) sẽ được chọn làm siêu tham số.
\end{itemize}
Từ quy trình trên, mô hình Ridge chọn được giá trị $\texttt{alpha}=33.6$.
\subsubsection{Kết luận}
Hồi quy Ridge, thông qua cơ chế chính quy hoá L2, được chọn như một giải pháp trực tiếp và hiệu quả cho vấn đề đa cộng tuyến. Bằng cách chấp nhận một lượng nhỏ độ lệch (\textit{bias}) thông qua việc co rút hệ số để từ đó giảm thiểu đáng kể được phương sai. Kết quả là một mô hình dự đoán ổn định, mạnh mẽ hơn và có khả năng tổng quát hoá tốt hơn trên dữ liệu mới so với mô hình tuyến tính cơ sở.
\subsection{Mô hình Lasso}
\subsubsection{Cơ sở lý thuyết}

Lasso (Least Absolute Shrinkage and Selection Operator) là một phương pháp hồi quy tuyến tính sử dụng chuẩn hóa L1 (L1 regularization). Kỹ thuật này đồng thời thực hiện lựa chọn biến (variable selection) và điều chuẩn hóa (regularization) để nâng cao độ chính xác dự báo và khả năng diễn giải của mô hình. Nói cách khác, Lasso bổ sung một khoản phạt L1 vào hàm mất mát của hồi quy thường, khiến một số hệ số hồi quy bị kéo về 0, từ đó đơn giản hóa mô hình và giúp tránh hiện tượng quá khớp. \cite{tibshirani1996lasso}

Hàm mục tiêu của hồi quy Lasso (với dữ liệu có $N$ mẫu, $p$ đặc trưng) có dạng:

\[
\min_{\beta_0,\boldsymbol{\beta}}
\; \sum_{i=1}^{N} \left( y_i - \beta_0 - \sum_{j=1}^{p} x_{ij}\beta_j \right)^2
\;+\; \lambda \sum_{j=1}^{p} \lvert \beta_j \rvert
\]


trong đó vế thứ nhất là lỗi bình phương trung bình (MSE) và vế thứ hai là khoản phạt $L1$ nhân với hệ số điều chuẩn $\lambda$. Tham số $\lambda$ (ký hiệu điều chuẩn, trong scikit-learn tham số này được gọi là \texttt{alpha}) kiểm soát độ mạnh của hình phạt $L1$, qua đó quyết định mức độ phức tạp của mô hình. $\lambda$ càng lớn thì mô hình bị phạt càng nhiều: các hệ số $\beta_j$ bị kéo về 0 đáng kể hơn, nhiều hệ số nhỏ gần như bị triệt tiêu; kết quả là mô hình giữ lại rất ít biến quan trọng (tránh overfitting). Ngược lại, $\lambda$ nhỏ chỉ phạt nhẹ, mô hình sẽ giữ lại nhiều đặc trưng hơn (gần với hồi quy thường). Tài liệu từ IBM mô tả: \textit{“Larger values of lambda increase the penalty, shrinking more of the coefficients towards zero; this subsequently reduces the importance of (or altogether eliminates) some of the features from the model, resulting in automatic feature selection. Conversely, smaller values of lambda reduce the effect of the penalty, retaining more features within the model.”}\cite{ibm-lasso} (Dịch: Giá trị $\lambda$ lớn làm tăng mức phạt, kéo nhiều hệ số hơn về gần 0; điều này làm giảm tầm quan trọng (hoặc thậm chí loại bỏ hoàn toàn) một số đặc trưng khỏi mô hình, dẫn đến việc tự động chọn lọc biến. Ngược lại, giá trị $\lambda$ nhỏ làm giảm ảnh hưởng của mức phạt, giữ lại nhiều đặc trưng hơn trong mô hình). Nhìn trên phương diện bias–variance, $\lambda$ đóng vai trò cân bằng giữa độ chệch và phương sai của mô hình. Cũng theo IBM: \textit{“As $\lambda$ increases, the bias increases, and the variance decreases, leading to a simpler model with fewer parameters. Conversely, as $\lambda$ decreases, the variance increases, leading to a more complex model with more parameters. If $\lambda$ is zero, then one is left with an OLS function – that is, a standard linear regression model without any regularization.”}\cite{ibm-lasso} (Dịch: Khi $\lambda$ tăng, bias tăng và variance giảm, mô hình đơn giản hơn với ít tham số hơn. Ngược lại, khi $\lambda$ giảm, variance tăng lên, mô hình phức tạp hơn với nhiều tham số hơn. Nếu $\lambda = 0$ thì hàm mục tiêu trở thành OLS – tức là mô hình hồi quy tuyến tính thông thường không có regularization). Trường hợp $\lambda = 0$ nghĩa là không áp dụng phạt, Lasso lúc này tương đương mô hình hồi quy thường và sẽ không có tác dụng chống overfitting; ngược lại, $\lambda$ quá lớn sẽ phạt mạnh đến mức hầu hết các hệ số bị triệt tiêu về 0, mô hình khi đó có thể bị underfitting (thiếu độ linh hoạt). Do đó, việc lựa chọn $\lambda$ tối ưu là rất quan trọng để mô hình đạt hiệu năng cao nhất.

Một ưu điểm nổi bật của Lasso là khả năng chọn lọc đặc trưng tự động nhờ vào chuẩn hóa $L1$. Khoản phạt $L1$ thúc đẩy nghiệm {\it thưa} (sparse solution), nhiều hệ số hồi quy có thể bị đẩy về đúng bằng 0. Điều này có nghĩa là mô hình Lasso sẽ loại bỏ hẳn những biến không quan trọng, chỉ giữ lại những biến thật sự có đóng góp lớn. Nói cách khác, Lasso vừa giảm overfitting vừa đơn giản hóa mô hình bằng cách bỏ qua các đặc trưng dư thừa. Như IBM mô tả: \textit{“Some variables will shrink exactly to zero, leaving the model with a subset of the most important variables to make predictions.”}\cite{ibm-lasso} (Dịch: Một số biến sẽ được kéo về đúng 0, khiến mô hình chỉ còn một tập hợp các biến quan trọng nhất để dự đoán). Nhờ đó, Lasso đặc biệt hữu ích khi xử lý dữ liệu có số lượng đặc trưng rất lớn hoặc có nhiều biến ít liên quan – mô hình sẽ tự động bỏ qua những biến ít liên quan, giảm nguy cơ overfitting và cải thiện tính diễn giải (model interpretability) do mô hình trở nên gọn nhẹ hơn.

\subsubsection{Triển khai thực nghiệm}
Trong thực nghiệm, chúng tôi xây dựng một pipeline gồm hai bước: (1) Chuẩn hóa dữ liệu bằng \texttt{StandardScaler} và (2) Hồi quy Lasso (scikit-learn). Việc chuẩn hóa thang đo các đặc trưng là cần thiết trước khi áp dụng Lasso, nhằm đảm bảo các hệ số bị phạt công bằng giữa các đặc trưng. Một blog khoa học dữ liệu nhấn mạnh: \textit{“It is crucial to scale (e.g. StandardScaler) input features because regression models are sensitive to them.”}\cite{mota-lasso} (Dịch: Việc chuẩn hóa các đặc trưng đầu vào (ví dụ dùng StandardScaler) là cực kỳ quan trọng vì các mô hình hồi quy rất nhạy cảm với đặc trưng có đơn vị hay độ lớn khác nhau). Nếu không chuẩn hóa, đặc trưng có độ lớn lớn sẽ bị phạt nặng hơn đặc trưng nhỏ, dẫn đến mức phạt $L1$ không đồng đều và ảnh hưởng xấu đến kết quả hồi quy Lasso. Do đó, toàn bộ features được chuẩn hóa về trung bình 0 và phương sai 1 trước khi huấn luyện mô hình.

Tiếp theo, để tìm giá trị điều chuẩn tối ưu cho mô hình Lasso, chúng tôi sử dụng phương pháp tìm kiếm lưới kết hợp cross-validation. Cụ thể, chúng tôi thực hiện \texttt{GridSearchCV} (5-fold cross-validation, scoring theo $R^2$) trên tham số $\alpha$ của Lasso (tương ứng với $\lambda$) trong khoảng logarithmic từ $10^{-4}$ đến $10^{3}$. Việc tìm kiếm trên không gian log-space giúp thử nhiều cấp độ regularization, từ rất nhẹ đến rất mạnh. Kết quả cho thấy $\alpha \approx 78.48$ là giá trị tối ưu cho mô hình (đạt $R^2$ cao nhất trên tập validation). Với $\alpha$ này, mô hình Lasso giữ lại được độ đơn giản cần thiết đồng thời vẫn giải thích tốt phương sai của dữ liệu. Mô hình cuối cùng được huấn luyện với \texttt{alpha=78.48}, \texttt{random\_state=42} (để kết quả tái lập) và \texttt{max\_iter=10000}. Việc tăng \texttt{max\_iter} lên 10000 vòng lặp nhằm đảm bảo thuật toán Lasso (coordinate descent) hội tụ, nhất là khi $\alpha$ khá lớn.

\subsubsection{Kết luận}
Lasso Regression với hình phạt $L1$ đã chứng tỏ vai trò hiệu quả trong việc kiểm soát hiện tượng overfitting và thực hiện chọn lọc đặc trưng tự động. Nhờ $\lambda$ được lựa chọn phù hợp, mô hình Lasso có thể giảm độ phức tạp (tránh overfit) bằng cách triệt tiêu các hệ số không cần thiết, đồng thời vẫn giữ được những đặc trưng quan trọng giúp mô hình dự báo chính xác. So với Ridge Regression, điểm khác biệt chính là Lasso tạo ra nghiệm thưa với một số hệ số đúng bằng 0 (tương ứng loại bỏ hoàn toàn những biến không quan trọng), trong khi Ridge (chuẩn hóa $L2$) không bao giờ đưa bất kỳ hệ số nào về 0 – do đó Ridge không có khả năng chọn lọc đặc trưng. Mặt khác, trong một số trường hợp, Ridge có thể cho kết quả tốt hơn Lasso nếu mô hình tối ưu không thực sự thưa hoặc khi các đặc trưng độc lập có tương quan cao. Khi các biến dự báo đều có đóng góp nhất định và tương quan với nhau, Lasso có xu hướng chỉ giữ lại ngẫu nhiên một biến rồi triệt tiêu các biến còn lại, có thể làm mất thông tin. Ngược lại, Ridge vẫn giữ lại tất cả các biến (mỗi biến đều có hệ số nhỏ hơn), nhờ đó tránh loại bỏ nhầm những biến hữu ích. Tóm lại, Lasso và Ridge đều là những kỹ thuật regularization hữu hiệu để giảm overfitting, nhưng Lasso tỏ ra ưu việt ở khả năng rút gọn mô hình và chọn lọc biến tự động, giúp mô hình vừa đơn giản vừa dễ diễn giải hơn.
\subsection{Mô hình Elastic Net}

Theo \cite{OverfittingMLCoBan2017}, khi kết hợp cả hai dạng regularization $l_1$ và $l_2$, ta thu được mô hình Elastic Net Regression.
Lúc đó, \textbf{hàm loss của Elastic Net} sẽ có dạng:
\[
J(w) = \frac{1}{2} \|y - Xw\|_2^2 + \lambda_1 \|w\|_1 + \lambda_2 \|w\|_2^2
\]
trong đó, $\lambda_1$ và $\lambda_2$ lần lượt là các hệ số điều chuẩn tương ứng với $l_1$ và $l_2$ regularization, giúp cân bằng giữa khả năng chọn lọc đặc trưng và việc giảm độ phức tạp của mô hình.
Nhờ đó, Elastic Net đặc biệt hữu ích khi dữ liệu vừa chứa nhiều đặc trưng không quan trọng, vừa tồn tại hiện tượng đa cộng tuyến (multicollinearity) giữa các biến.


\subsection{Phân tích dữ liệu và lựa chọn mô hình}
Ta tiến hành trích xuất một số đặc trưng của các cột dữ liệu (trừ carname) của dataset:
\begin{figure}[H]
\centering
\includegraphics[scale=1]{img/corr_features_heatmap.png}
\caption{Heatmap correlation giữa các feature}
\label{fig:corr_features_heatmap}
\end{figure}

Có 8 cặp feature có correlation cao được highlight trên hình như là carlength với wheelbase, carwidth, curbweight; carwidth và curbweight, enginesize và curbweight, horsepower và enginesize, citympg và horsepower, và highwaympg với citympg.

\begin{figure}[H]
\centering
\includegraphics[scale=0.85]{img/feature_target_corr.png}
\caption{Bar chart correlation giữa feature và target (price)}
\label{fig:feature_target_corr}
\end{figure}

Ngược lại, khi kiểm tra correlation giữa các feature và target, ta thấy có những biến có correlation rất là thấp: carhieght, car\_ID, peakrpm, symboling, stroke, và compressionratio.

=> Đây là một dataset vừa có các feature có correlation với nhau rất cao, vừa có các feature khác có correlation với target gần như bằng 0.

Với vấn đề đầu tiên, ta có thể giải quyết bằng Ridge. Với vấn đề thứ 2, ta có thể giải quyết bằng Lasso. Ngoài ra, thay vì thêm một chuẩn bậc nhất hoặc bậc 2 vào hàm loss nhằm giải quyết các vấn đề liên quan đến dataset, ta cũng có thể thử tối ưu hóa dataset ngay từ bước đầu tiên thông qua việc chọn các feature cho phù hợp.

Vì vậy, nhóm quyết định sẽ thử chạy dataset này thông qua các model: Ridge, Lasso, Elastic Net (Ridge + Lasso), và Linear Regression cơ bản nhưng các feature được chọn trước thông qua thuật toán Forward Selection BIC.

Với các model: Ridge, Lasso, Elastic, hệ số $\lambda_1$ và $\lambda_2$ được chọn đơn giản bằng grid search với target là $R^2$ cao nhất.